{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f0ef7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing source: Timo in ../data_cleaned/Timo\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-11.parquet\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-11.csv\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-10.parquet\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-10.csv\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-08.parquet\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-08.csv\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-14.parquet\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-14.csv\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-12.parquet\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-12.csv\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-07.parquet\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-07.csv\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-09.parquet\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-09.csv\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-13.parquet\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-13.csv\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-06.parquet\n",
      "Wrote ../data_cleaned/parquet_outputs/Timo/output-2025-06-06.csv\n",
      "Wrote master Parquet: ../data_cleaned/parquet_outputs/Timo/output-all.parquet\n",
      "Wrote master CSV: ../data_cleaned/parquet_outputs/Timo/output-all.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration: folder containing the zip files\n",
    "DATA_PARENT_DIR = '../data_cleaned/'\n",
    "SOURCE_DIR = 'Timo'\n",
    "DATA_DIR = os.path.join(DATA_PARENT_DIR, SOURCE_DIR)\n",
    "\n",
    "# NEW: define where to write the Parquet outputs\n",
    "# e.g. OUTPUT_DIR = '../data_processed/parquet_outputs'\n",
    "OUTPUT_DIR = os.path.join(DATA_PARENT_DIR, 'parquet_outputs', SOURCE_DIR)\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Pattern to identify and parse filenames\n",
    "FNAME_REGEX = re.compile(r'^(?P<label>base|(?P<intake>\\d+)ml)\\s+(?P<date>\\d{4}-\\d{2}-\\d{2})')\n",
    "\n",
    "# Sensor file names inside each zip\n",
    "SENSOR_FILES = {\n",
    "    'Accelerometer.csv': 'accelerometer',\n",
    "    'Gyroscope.csv': 'gyroscope',\n",
    "    'Linear Acceleration.csv': 'linear_acceleration'\n",
    "}\n",
    "\n",
    "# Collect data per experiment date\n",
    "daily_data = {}\n",
    "\n",
    "# Walk through DATA_DIR to include subfolders (sources)\n",
    "for root, dirs, files in os.walk(DATA_DIR):\n",
    "    source = SOURCE_DIR\n",
    "    print(f\"Processing source: {source} in {root}\")\n",
    "\n",
    "    for fname in files:\n",
    "        if not fname.lower().endswith('.zip'):\n",
    "            continue\n",
    "\n",
    "        match = FNAME_REGEX.match(fname)\n",
    "        if not match:\n",
    "            print(f\"Skipping unknown-format file: {fname} in {root}\")\n",
    "            continue\n",
    "\n",
    "        info = match.groupdict()\n",
    "        date_key = info['date']\n",
    "        is_base = 1 if info['label'] == 'base' else 0\n",
    "        intake = 0 if is_base else int(info['intake'])\n",
    "\n",
    "        zip_path = os.path.join(root, fname)\n",
    "        dfs = []\n",
    "\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            for internal_name, sensor_key in SENSOR_FILES.items():\n",
    "                try:\n",
    "                    with z.open(internal_name) as f:\n",
    "                        df = pd.read_csv(f)\n",
    "                except KeyError:\n",
    "                    print(f\"  - Warning: {internal_name} not found in {fname}\")\n",
    "                    continue\n",
    "\n",
    "                # Rename x/y/z columns to include sensor prefix\n",
    "                rename_map = {\n",
    "                    col: f\"{sensor_key}_{col}\"\n",
    "                    for col in ['x', 'y', 'z']\n",
    "                    if col in df.columns\n",
    "                }\n",
    "                if rename_map:\n",
    "                    df = df.rename(columns=rename_map)\n",
    "\n",
    "                dfs.append(df)\n",
    "\n",
    "        if not dfs:\n",
    "            print(f\"No sensor data for {fname}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Merge all sensor data on 'time'\n",
    "        merged = dfs[0]\n",
    "        for df in dfs[1:]:\n",
    "            merged = pd.merge(merged, df, on='time', how='outer')\n",
    "\n",
    "        # Add experiment metadata\n",
    "        merged['experiment_id'] = date_key\n",
    "        merged['base'] = is_base\n",
    "        merged['caffeine_ml'] = intake\n",
    "        merged['source'] = source\n",
    "\n",
    "        # Reorder columns\n",
    "        cols = [\n",
    "            'experiment_id', 'base', 'caffeine_ml', 'source', 'time'\n",
    "        ] + [\n",
    "            c for c in merged.columns\n",
    "            if c not in ['experiment_id', 'base', 'caffeine_ml', 'source', 'time']\n",
    "        ]\n",
    "        merged = merged[cols]\n",
    "\n",
    "        daily_data.setdefault(date_key, []).append(merged)\n",
    "\n",
    "# Write combined Parquet per day into OUTPUT_DIR\n",
    "for date_key, frames in daily_data.items():\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    out_fname = f\"output-{date_key}.parquet\"\n",
    "    out_path = os.path.join(OUTPUT_DIR, out_fname)\n",
    "    combined.to_parquet(out_path, index=False, compression='snappy')\n",
    "    print(f\"Wrote {out_path}\")\n",
    "\n",
    "    out_csv = os.path.join(OUTPUT_DIR, f\"output-{date_key}.csv\")\n",
    "    combined.to_csv(out_csv, index=False)\n",
    "    print(f\"Wrote {out_csv}\")\n",
    "\n",
    "\n",
    "# 2) Write master Parquet and master CSV combining all days\n",
    "all_frames = [df for frames in daily_data.values() for df in frames]\n",
    "if all_frames:\n",
    "    all_combined = pd.concat(all_frames, ignore_index=True)\n",
    "\n",
    "    # Master Parquet\n",
    "    master_parquet = os.path.join(OUTPUT_DIR, 'output-all.parquet')\n",
    "    all_combined.to_parquet(master_parquet, index=False, compression='snappy')\n",
    "    print(f\"Wrote master Parquet: {master_parquet}\")\n",
    "\n",
    "    # Master CSV\n",
    "    master_csv = os.path.join(OUTPUT_DIR, 'output-all.csv')\n",
    "    all_combined.to_csv(master_csv, index=False)\n",
    "    print(f\"Wrote master CSV: {master_csv}\")\n",
    "else:\n",
    "    print(\"No data available to write master files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837066c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
