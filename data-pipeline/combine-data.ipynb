{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ef7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing source: Timo in ../data_cleaned/Timo\n",
      "Wrote ../data_cleaned/Timo/output-2025-06-11.csv\n",
      "Wrote ../data_cleaned/Timo/output-2025-06-10.csv\n",
      "Wrote ../data_cleaned/Timo/output-2025-06-08.csv\n",
      "Wrote ../data_cleaned/Timo/output-2025-06-14.csv\n",
      "Wrote ../data_cleaned/Timo/output-2025-06-12.csv\n",
      "Wrote ../data_cleaned/Timo/output-2025-06-07.csv\n",
      "Wrote ../data_cleaned/Timo/output-2025-06-09.csv\n",
      "Wrote ../data_cleaned/Timo/output-2025-06-13.csv\n",
      "Wrote ../data_cleaned/Timo/output-2025-06-06.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration: folder containing the zip files\n",
    "DATA_PARENT_DIR = '../data_cleaned/'\n",
    "SOURCE_DIR = 'Timo'\n",
    "DATA_DIR = os.path.join(DATA_PARENT_DIR, SOURCE_DIR)\n",
    "\n",
    "# Pattern to identify and parse filenames\n",
    "# Examples:\n",
    "#   base 2025-06-14 12-25-22_trimmed.zip\n",
    "#   360ml 2025-06-14 12-57-02_trimmed.zip\n",
    "FNAME_REGEX = re.compile(r'^(?P<label>base|(?P<intake>\\d+)ml)\\s+(?P<date>\\d{4}-\\d{2}-\\d{2})')\n",
    "\n",
    "# Sensor file names inside each zip\n",
    "SENSOR_FILES = {\n",
    "    'Accelerometer.csv': 'accelerometer',\n",
    "    'Gyroscope.csv': 'gyroscope',\n",
    "    'Linear Acceleration.csv': 'linear_acceleration'\n",
    "}\n",
    "\n",
    "# Collect data per experiment date\n",
    "daily_data = {}\n",
    "\n",
    "# Walk through DATA_DIR to include subfolders (sources)\n",
    "for root, dirs, files in os.walk(DATA_DIR):\n",
    "    # Determine source name as the immediate folder name relative to DATA_DIR\n",
    "    rel_path = os.path.relpath(root, DATA_DIR)\n",
    "    source = SOURCE_DIR\n",
    "\n",
    "    print(f\"Processing source: {source} in {root}\")\n",
    "\n",
    "    for fname in files:\n",
    "        if not fname.lower().endswith('.zip'):\n",
    "            continue\n",
    "\n",
    "        match = FNAME_REGEX.match(fname)\n",
    "        if not match:\n",
    "            print(f\"Skipping unknown-format file: {fname} in {root}\")\n",
    "            continue\n",
    "\n",
    "        info = match.groupdict()\n",
    "        date_key = info['date']           # experiment_id\n",
    "        is_base = 1 if info['label'] == 'base' else 0\n",
    "        intake = 0 if is_base else int(info['intake'])\n",
    "\n",
    "        zip_path = os.path.join(root, fname)\n",
    "\n",
    "        # Read each sensor CSV from the zip\n",
    "        dfs = []\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            for internal_name, sensor_key in SENSOR_FILES.items():\n",
    "                try:\n",
    "                    with z.open(internal_name) as f:\n",
    "                        df = pd.read_csv(f)\n",
    "                except KeyError:\n",
    "                    print(f\"  - Warning: {internal_name} not found in {fname}\")\n",
    "                    continue\n",
    "\n",
    "                # Rename x/y/z columns to include sensor prefix\n",
    "                rename_map = {col: f\"{sensor_key}_{col}\"\n",
    "                              for col in ['x', 'y', 'z']\n",
    "                              if col in df.columns}\n",
    "                if rename_map:\n",
    "                    df = df.rename(columns=rename_map)\n",
    "\n",
    "                dfs.append(df)\n",
    "\n",
    "        if not dfs:\n",
    "            print(f\"No sensor data for {fname}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Merge all sensor data on 'time'\n",
    "        merged = dfs[0]\n",
    "        for df in dfs[1:]:\n",
    "            merged = pd.merge(merged, df, on='time', how='outer')\n",
    "\n",
    "        # Add experiment metadata\n",
    "        merged['experiment_id'] = date_key\n",
    "        merged['base'] = is_base\n",
    "        merged['caffeine_ml'] = intake\n",
    "        merged['source'] = source\n",
    "\n",
    "        # Reorder columns: experiment_id, base, caffeine_ml, source, time, sensors...\n",
    "        cols = ['experiment_id', 'base', 'caffeine_ml', 'source', 'time'] + \\\n",
    "               [c for c in merged.columns if c not in ['experiment_id', 'base', 'caffeine_ml', 'source', 'time']]\n",
    "        merged = merged[cols]\n",
    "\n",
    "        # Store for daily aggregation\n",
    "        daily_data.setdefault(date_key, []).append(merged)\n",
    "\n",
    "# Write combined CSV per day\n",
    "for date_key, frames in daily_data.items():\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    out_fname = f\"output-{date_key}.csv\"\n",
    "    out_path = os.path.join(DATA_DIR, out_fname)\n",
    "    combined.to_csv(out_path, index=False)\n",
    "    print(f\"Wrote {out_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
